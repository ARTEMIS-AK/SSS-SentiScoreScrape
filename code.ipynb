{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8P0XbWGnGTGn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now downloading required libraries"
      ],
      "metadata": {
        "id": "8p8nqlLrqs8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y-bXhEiGeEZ",
        "outputId": "d2f7031d-8fa2-4e34-a5a3-ad5ecf763cd4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to count syllables in a word"
      ],
      "metadata": {
        "id": "uEt3m9Ruq4wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables(word):\n",
        "    vowels = 'aeiouy'\n",
        "    count = 0\n",
        "    word = word.lower()\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith('e'):\n",
        "        count -= 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "q175hiqPGmry"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlWf8s0GsUcV",
        "outputId": "c67bea72-557e-4f2d-9d8d-0580ddeb288c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading excel file using pandas"
      ],
      "metadata": {
        "id": "kVUopCKdq91N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = pd.read_csv('/content/drive/MyDrive/BlackCoffer/Coffer2/Input.xlsx - Sheet1.csv')"
      ],
      "metadata": {
        "id": "bzLTOL0mGsF_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looping through each row in the input data\n",
        "for index, row in input_data.iterrows():\n",
        "    url = row['URL']\n",
        "    url_id = row['URL_ID']\n",
        "\n",
        "\n",
        "    # Fetching data from the specified URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "\n",
        "        # Parsing HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "\n",
        "        # Writing the title and article text to a text file\n",
        "        with open(f\"{url_id}.txt\", 'w', encoding='utf-8') as file:\n",
        "            title = soup.find('title').get_text()\n",
        "            article_text = \" \".join([p.get_text() for p in soup.find_all('p')])\n",
        "\n",
        "            file.write(f\"{title}\\n\\n{article_text}\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch data from {url}\")\n",
        "\n",
        "print(\"Data extraction complete.\")"
      ],
      "metadata": {
        "id": "_ChFZQkQHMc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00c6ae2-1a64-40de-fbee-404d2c666fb8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch data from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Failed to fetch data from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Data extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "output_data = []\n",
        "\n",
        "# Looping through each row in the input data again\n",
        "for index, row in input_data.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    filename = f\"{url_id}.txt\"\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Tokenizing sentences and words from the text content\n",
        "        sentences = sent_tokenize(content)\n",
        "        words = word_tokenize(content)\n",
        "        num_sentences = len(sentences)\n",
        "        num_words = len(words)\n",
        "        avg_sentence_length = num_words / num_sentences\n",
        "\n",
        "        # Calculating average sentence length\n",
        "        avg_sentence_length = len(words) / len(sentences)\n",
        "\n",
        "        # Analyzing text sentiment using TextBlob\n",
        "        blob = TextBlob(content)\n",
        "        polarity = blob.sentiment.polarity\n",
        "        subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "        # Calculating various text metrics\n",
        "        complex_word_count = sum(count_syllables(word) > 3 for word in words)\n",
        "        percentage_complex_words = (complex_word_count / num_words) * 100 if num_words > 0 else 0\n",
        "\n",
        "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "\n",
        "        word_count = len(words)\n",
        "\n",
        "        positive_score = (polarity + 1) / 2\n",
        "        negative_score = (1 - polarity) / 2\n",
        "\n",
        "        overall_polarity = abs(polarity)\n",
        "\n",
        "        syllables_per_word = sum(count_syllables(word) for word in words) / num_words if num_words > 0 else 0\n",
        "\n",
        "        personal_pronouns = sum(1 for word in words if word.lower() in ['i', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'we', 'us', 'our', 'ours', 'ourselves', 'they', 'them', 'their', 'theirs', 'themselves'])\n",
        "\n",
        "        avg_word_length = sum(len(word) for word in words) / num_words if num_words > 0 else 0\n",
        "\n",
        "        # Appending metrics to the output data list\n",
        "        output_data.append({\n",
        "                'URL_ID': url_id,\n",
        "                'POSITIVE SCORE': positive_score,\n",
        "                'NEGATIVE SCORE': negative_score,\n",
        "                'POLARITY SCORE': overall_polarity,\n",
        "                'SUBJECTIVITY SCORE': subjectivity,\n",
        "                'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "                'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "                'FOG INDEX': fog_index,\n",
        "                'AVG NUMBER OF WORDS PER SENTENCE': avg_sentence_length,\n",
        "                'COMPLEX WORD COUNT': complex_word_count,\n",
        "                'WORD COUNT': num_words,\n",
        "                'SYLLABLE PER WORD': syllables_per_word,\n",
        "                'PERSONAL PRONOUNS': personal_pronouns,\n",
        "                'AVG WORD LENGTH': avg_word_length\n",
        "            })\n",
        "    else:\n",
        "        print(f\"File {filename} does not exist. Skipping...\")\n"
      ],
      "metadata": {
        "id": "jmBdsb5vH4Vg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f97e1a-9aa7-4123-c19f-5b9ea06b7073"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File blackassign0036.txt does not exist. Skipping...\n",
            "File blackassign0049.txt does not exist. Skipping...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating a DataFrame from the output data\n",
        "output_df = pd.DataFrame(output_data)\n",
        "\n",
        "# Merging input data with calculated metrics\n",
        "final_output = pd.merge(input_data, output_df, on='URL_ID')\n",
        "\n",
        "# Saving the final output to an Excel file\n",
        "final_output.to_excel('output1.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis complete. Output saved in output.xlsx.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOWcu0wsJipB",
        "outputId": "39e57322-9351-4571-fdf0-8a29f1041eda"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text analysis complete. Output saved in output.xlsx.\n"
          ]
        }
      ]
    }
  ]
}
